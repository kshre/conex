14684/14684 [==============================] - 111s - loss: 0.0418     
Epoch 6/20
14684/14684 [==============================] - 111s - loss: 0.0411     
Epoch 7/20
14684/14684 [==============================] - 111s - loss: 0.0391     
Epoch 9/20
14684/14684 [==============================] - 111s - loss: 0.0379     
Epoch 10/20
14684/14684 [==============================] - 111s - loss: 0.0368     
Epoch 11/20
14684/14684 [==============================] - 111s - loss: 0.0357     
Epoch 12/20
14684/14684 [==============================] - 111s - loss: 0.0343     
Epoch 13/20
14684/14684 [==============================] - 111s - loss: 0.0329     
Epoch 14/20
14684/14684 [==============================] - 112s - loss: 0.0316     
Epoch 15/20
14684/14684 [==============================] - 111s - loss: 0.0303     
Epoch 16/20
14684/14684 [==============================] - 111s - loss: 0.0292     
Epoch 17/20
14684/14684 [==============================] - 111s - loss: 0.0287     
Epoch 18/20
14684/14684 [==============================] - 111s - loss: 0.0273     
Epoch 19/20
14684/14684 [==============================] - 111s - loss: 0.0268     
Epoch 20/20
14684/14684 [==============================] - 111s - loss: 0.0265     
training done


train
          0    1    2    3    4    5    6  (gold)
 0     103338 3784 2918 3161 4912 2281 2433
 1       86 2282   29   35   74    1    2
 2       48    9 1072   28    0   46    3
 3       33   15   22 1138    1    2   37
 4      323  208   12   12 4266   15   13
 5      111    7   80    2   21 1221   12
 6       71    4    2   55    9    6 1352
(pred)




dev
          0    1    2    3    4    5    6  (gold)
 0     10188  706  448  382  901  298  228
 1      169    9    1    8   20    2    5
 2       75    7    1    2    5    1    2
 3       74   10    3    2    8    3    3
 4      253   21   12   10   29    7    7
 5       73    5    5    2    1    0    2
 6       66    5    2    4    6    3    0
(pred)



serializing model to models/word-ls


